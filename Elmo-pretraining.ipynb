{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/turning/anaconda3/envs/torchy/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/turning/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/turning/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing the libraries \n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch import cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the CONSTANTS \n",
    "\n",
    "EXCLUDE_STOPWORDS = True\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "EMBEDDING_DIM = 100 \n",
    "HIDDEN_DIM = 100\n",
    "GLOVE_PATH = 'glove/glove.6B.100d.txt'\n",
    "DEVICE = 'cuda'\n",
    "if cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst (/home/turning/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n",
      "100%|██████████| 3/3 [00:00<00:00, 156.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# downloading the dataset and loading the glove embeddings \n",
    "dataset = load_dataset(\"sst\", \"default\")\n",
    "glove = {}\n",
    "with open(GLOVE_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        glove[line[0]] = torch.tensor([float(x) for x in line[1:]])\n",
    "\n",
    "# create a list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "glove['<unk>'] = torch.mean(torch.stack(list(glove.values())), dim=0)\n",
    "glove['<pad>'] = torch.zeros(EMBEDDING_DIM)\n",
    "glove['<start>'] = torch.rand(EMBEDDING_DIM)\n",
    "glove['<end>'] = torch.rand(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the word_2_idx and idx_2_word dictionaries and the embedding matrix\n",
    "word_2_idx = {'<pad>': 0, '<unk>': 1, '<start>': 2, '<end>': 3}\n",
    "idx_2_word = {0: '<pad>', 1: '<unk>', 2: '<start>', 3: '<end>'}\n",
    "embedding_matrix = np.zeros((len(glove.values()), EMBEDDING_DIM))\n",
    "embedding_matrix[0] = glove['<pad>']\n",
    "embedding_matrix[1] = glove['<unk>']\n",
    "embedding_matrix[2] = glove['<start>']\n",
    "embedding_matrix[3] = glove['<end>']\n",
    "\n",
    "for i, word in enumerate(glove.keys()):\n",
    "    if word not in word_2_idx:\n",
    "        word_2_idx[word] = len(word_2_idx)\n",
    "        idx_2_word[len(idx_2_word)] = word\n",
    "        embedding_matrix[word_2_idx[word]] = glove[word]\n",
    "\n",
    "# convert the embedding matrix to a tensor\n",
    "embedding_matrix = torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defing the model which we are going to pretrain\n",
    "class ELMo(nn.Module):\n",
    "    '''this class implements the ELMo model using the BI-LSTM architecture like by stacking two LSTM layers'''\n",
    "    def __init__(self, embedding_dim, vocab_size,  hidden_dim1, hidden_dim2 ,batch_size, num_layers=2):\n",
    "        super(ELMo, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.vocb_size =  vocab_size\n",
    "        self.embedding= nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1*2, hidden_dim2, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_dim2*2, vocab_size)\n",
    "        self.weight1 = nn.Parameter(torch.randn(1))\n",
    "        self.weight2 = nn.Parameter(torch.randn(1))\n",
    "        self.lambda1 = nn.Parameter(torch.randn(1))\n",
    "\n",
    "\n",
    "    def forward(self, input): \n",
    "        # input = [batch_size, seq_len]\n",
    "        # getting the imput embeddings \n",
    "        input_embeddings = self.embedding(input) # [batch_size, seq_len, embedding_dim]\n",
    "        # passing the embeddings to the first LSTM layer\n",
    "        output1 , (hidden1, cell1) = self.lstm1(input_embeddings) # [batch_size, seq_len, hidden_dim1]\n",
    "\n",
    "        # passing the output of the first LSTM layer to the second LSTM layer\n",
    "        output2 , (hidden2, cell2) = self.lstm2(output1) # [batch_size, seq_len, hidden_dim2]\n",
    "        # adding the two outputs of the LSTM layers\n",
    "        \n",
    "        weighted_output = self.lambda1*((self.weight1 * output1) +( self.weight2 * output2))\n",
    "    \n",
    "        # output = [batch_size, seq_len, vocab_size]\n",
    "        output = self.linear(weighted_output)\n",
    "        \n",
    "        output_softmax = F.log_softmax(output, dim=2)\n",
    "        # removing the last token from the output as we are pretraing the model \n",
    "        output_softmax = output_softmax.permute(0,2,1)[:,:,:-1]\n",
    "\n",
    "        return output_softmax\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the datasets like tokenising them \n",
    "prediction_raw_datasets={}\n",
    "prediction_raw_datasets['train'] = [ i.lower().split('|') for i in dataset['train']['tokens']]\n",
    "prediction_raw_datasets['validation'] = [ i.lower().split('|') for i in dataset['validation']['tokens']]\n",
    "prediction_raw_datasets['test'] = [ i.lower().split('|') for i in dataset['test']['tokens']]\n",
    "\n",
    "for k , v in prediction_raw_datasets.items():\n",
    "    for i in range(len(v)):\n",
    "        if EXCLUDE_STOPWORDS:\n",
    "            v[i] = [word for word in v[i] if word not in stop_words]\n",
    "        for j in range(len(v[i])):\n",
    "            if v[i][j] not in word_2_idx:\n",
    "                v[i][j] = '<unk>'\n",
    "\n",
    "        v[i]= ['<start>'] + v[i] + ['<end>']\n",
    "        v[i] = [word_2_idx[word] for word in v[i]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the datasets with sentence and label\n",
    "datasets = {'train': [], 'validation': [], 'test': []}\n",
    "for i in range(len(prediction_raw_datasets['train'])):  \n",
    "    sentence = torch.LongTensor(prediction_raw_datasets['train'][i])                                        \n",
    "    datasets['train'].append({'sentence': sentence, 'label': sentence[:-1]})\n",
    "for i in range(len(prediction_raw_datasets['validation'])):  \n",
    "    sentence = torch.LongTensor(prediction_raw_datasets['validation'][i])                                        \n",
    "    datasets['validation'].append({'sentence': sentence, 'label': sentence[:-1]})\n",
    "for i in range(len(prediction_raw_datasets['test'])):\n",
    "    sentence = torch.LongTensor(prediction_raw_datasets['test'][i])                                        \n",
    "    datasets['test'].append({'sentence': sentence, 'label': sentence[:-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definig the obejct model\n",
    "model = ELMo(EMBEDDING_DIM, len(glove), HIDDEN_DIM, EMBEDDING_DIM//2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "steps:  100 train loss:  10.736334671974182 validation loss:  tensor(10.4804) validation accuracy:  tensor(0.0841)\n",
      "steps:  200 train loss:  8.983240189552307 validation loss:  tensor(10.4242) validation accuracy:  tensor(0.1321)\n",
      "steps:  300 train loss:  7.884680528640747 validation loss:  tensor(11.3775) validation accuracy:  tensor(0.0726)\n",
      "steps:  400 train loss:  7.491342635154724 validation loss:  tensor(11.6259) validation accuracy:  tensor(0.0104)\n",
      "steps:  500 train loss:  7.289140355587006 validation loss:  tensor(11.8951) validation accuracy:  tensor(0.0097)\n",
      "steps:  600 train loss:  7.006893243789673 validation loss:  tensor(12.2087) validation accuracy:  tensor(0.0082)\n",
      "steps:  700 train loss:  7.318323919773102 validation loss:  tensor(12.1857) validation accuracy:  tensor(0.0109)\n",
      "steps:  800 train loss:  7.035793871879577 validation loss:  tensor(12.2476) validation accuracy:  tensor(0.0064)\n",
      "steps:  900 train loss:  6.9275182390213015 validation loss:  tensor(12.3006) validation accuracy:  tensor(0.0112)\n",
      "steps:  1000 train loss:  6.7818870043754576 validation loss:  tensor(12.5968) validation accuracy:  tensor(0.0109)\n",
      "steps:  1100 train loss:  6.610476016998291 validation loss:  tensor(12.8090) validation accuracy:  tensor(0.0078)\n",
      "steps:  1200 train loss:  6.239480238780379 validation loss:  tensor(12.4444) validation accuracy:  tensor(0.0093)\n",
      "steps:  1300 train loss:  6.548174817562103 validation loss:  tensor(12.4432) validation accuracy:  tensor(0.0127)\n",
      "steps:  1400 train loss:  6.063463728427887 validation loss:  tensor(12.4609) validation accuracy:  tensor(0.0110)\n",
      "steps:  1500 train loss:  6.128679357394576 validation loss:  tensor(12.3277) validation accuracy:  tensor(0.0122)\n",
      "steps:  1600 train loss:  6.134884822983294 validation loss:  tensor(12.5426) validation accuracy:  tensor(0.0089)\n",
      "steps:  1700 train loss:  5.899848631620407 validation loss:  tensor(12.5551) validation accuracy:  tensor(0.0098)\n",
      "steps:  1800 train loss:  6.115428035259247 validation loss:  tensor(12.4253) validation accuracy:  tensor(0.0086)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m     42\u001b[0m label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 43\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     44\u001b[0m val_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m criterion(output, label)\n\u001b[1;32m     45\u001b[0m val_accuracy \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m accuracy(output, label)\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mELMo.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m# adding the two outputs of the LSTM layers\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[39m# output = [batch_size, seq_len, vocab_size]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput2)\n\u001b[0;32m---> 30\u001b[0m output_softmax \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mlog_softmax(output, dim\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     31\u001b[0m \u001b[39m# removing the last token from the output as we are pretraing the model \u001b[39;00m\n\u001b[1;32m     32\u001b[0m output_softmax \u001b[39m=\u001b[39m output_softmax\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m)[:,:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/torchy/lib/python3.10/site-packages/torch/nn/functional.py:1923\u001b[0m, in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1921\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39mlog_softmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1922\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1923\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mlog_softmax(dim)\n\u001b[1;32m   1924\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1925\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mlog_softmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.to(DEVICE)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# define the optimizer \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "best_loss = 1000000\n",
    "best_accuracy = 0\n",
    "def accuracy(output, label):\n",
    "    output = output.argmax(dim=1)\n",
    "    return (output == label).float().mean()\n",
    "steps = 0\n",
    "\n",
    "running_loss = 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    print('epoch: ', epoch)\n",
    "    if epoch%3 == 0 and epoch != 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr']/2\n",
    "    for i in range(len(datasets['train'])):\n",
    "        steps += 1\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        input = datasets['train'][i]['sentence'].unsqueeze(0)\n",
    "        label = datasets['train'][i]['label'].unsqueeze(0)\n",
    "        input = input.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        output = model.forward(input)\n",
    "        loss = criterion(output, label)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if steps%100 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                val_accuracy = 0\n",
    "                for j in range(len(datasets['validation'])):\n",
    "                    input = datasets['validation'][j]['sentence'].unsqueeze(0)\n",
    "                    label = datasets['validation'][j]['label'].unsqueeze(0)\n",
    "                    input = input.to(DEVICE)\n",
    "                    label = label.to(DEVICE)\n",
    "                    output = model.forward(input)\n",
    "                    val_loss += criterion(output, label)\n",
    "                    val_accuracy += accuracy(output, label)\n",
    "                val_loss = val_loss/len(datasets['validation'])\n",
    "                val_accuracy = val_accuracy/len(datasets['validation'])\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    torch.save(model.state_dict(), 'best_loss.pth')\n",
    "                if val_accuracy > best_accuracy:\n",
    "                    best_accuracy = val_accuracy\n",
    "                    torch.save(model.state_dict(), 'best_accuracy.pth')\n",
    "                print('steps: ', steps, 'train loss: ', running_loss/100, 'validation loss: ', val_loss, 'validation accuracy: ', val_accuracy)\n",
    "                running_loss = 0\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# now that we have trained the model we can get the weighted outputs of the two LSTM layers\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfor\u001b[39;00m parametrs \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparameters():\n\u001b[1;32m      4\u001b[0m     parametrs\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39munfreeze_weights()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# now that we have trained the model we can get the weighted outputs of the two LSTM layers\n",
    "for parametrs in model.parameters():\n",
    "    parametrs.requires_grad = False\n",
    "model.unfreeze_weights()\n",
    "dataset_for_sa = {'train': [], 'validation': [], 'test': []}\n",
    "for i in range(len(prediction_raw_datasets['train'])):  \n",
    "    sentence = torch.LongTensor(prediction_raw_datasets['train'][i])                                        \n",
    "    dataset_for_sa['train'].append({'sentence': sentence, 'label': dataset['train']['label'][i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pretrained_model = model\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(EMBEDDING_DIM,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 1)\n",
    ")\n",
    "\n",
    "# Define the combind model\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, pretrained_model, mlp):\n",
    "        super().__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.mlp = mlp\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pretrained_model(x)\n",
    "        x = x.mean(dim=1) \n",
    "        # x = [batch_size, embedding_dim]\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output of the pretrained model\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "# Create the combined model\n",
    "combined_model = CombinedModel(pretrained_model, mlp)\n",
    "\n",
    "# Define the loss function and optimization method\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(mlp.parameters())\n",
    "\n",
    "# Train the combined model\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Test the combined model\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = combined_model(inputs)\n",
    "        # Compute accuracy and other metrics as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
